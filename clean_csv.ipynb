{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dfd5008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There are 1 GPU(s) available.\n",
      "We will use the GPU: NVIDIA GeForce RTX 3060\n",
      "Found 4 dataset files to process...\n",
      "--> Successfully loaded and processed 'data-augmented.csv', adding 5572 rows.\n",
      "--> Successfully loaded and processed 'sms_spam.csv', adding 5574 rows.\n",
      "--> Successfully loaded and processed 'SMS_Text.csv', adding 5574 rows.\n",
      "--> Successfully loaded and processed 'spam_sms.csv', adding 5572 rows.\n",
      "\n",
      "Total combined rows: 22,292\n",
      "Rows after removing duplicate text entries: 16,205\n",
      "Final dataset shuffled.\n",
      "\n",
      "--- Final Class Distribution ---\n",
      "Ham (0):  14,016\n",
      "Spam (1): 2,189\n",
      "--------------------------------\n",
      "\n",
      "Successfully saved the cleaned dataset to 'cleaned_spam_dataset_v1_v2.csv' with columns v1 and v2.\n",
      "\n",
      "--- Final DataFrame Head (with v1, v2 columns) ---\n",
      "   v1                                                 v2\n",
      "0   0  ['Theyre doing it to lots of places. Only hosp...\n",
      "1   0  Buzz! Hey, my Love ! I think of you and hope y...\n",
      "2   0  ['NO GIFTS!! You trying to get me to throw mys...\n",
      "3   0  No dear i do have free messages without any re...\n",
      "4   1  Promotion Number: 8714714 - UR awarded a City ...\n",
      "5   0  [\"What is this 'hex' place you talk of? Explai...\n",
      "6   0  [\"That's y i said it's bad dat all e gals know...\n",
      "7   0  Reading gud habit.. Nan bari hudgi yorge patai...\n",
      "8   0  Omg you can make a wedding chapel in frontierv...\n",
      "9   0             Yeah I'll try to scrounge something up\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import os\n",
    "import torch\n",
    "\n",
    "\n",
    "# --- Configuration & Hyperparameters ---\n",
    "MODEL_NAME = './local-bert-base-uncased'\n",
    "DATASET_DIRECTORY = './datasets/'\n",
    "OUTPUT_FILENAME = 'cleaned_spam_dataset.csv'\n",
    "\n",
    "\n",
    "# --- Device Setup ---\n",
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(f'There are {torch.cuda.device_count()} GPU(s) available.')\n",
    "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
    "else:\n",
    "    print('No GPU available, using the CPU instead.')\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "def load_and_standardize_datasets(path, output_path):\n",
    "    \"\"\"\n",
    "    Loads, standardizes, combines, cleans, shuffles, and saves datasets.\n",
    "    Also prints the final class distribution.\n",
    "    \"\"\"\n",
    "    all_files = glob.glob(os.path.join(path, \"*.csv\"))\n",
    "    if not all_files:\n",
    "        print(f\"Error: No CSV files were found in the directory '{path}'.\")\n",
    "        exit()\n",
    "\n",
    "    print(f\"Found {len(all_files)} dataset files to process...\")\n",
    "\n",
    "    df_list = []\n",
    "\n",
    "\n",
    "\n",
    "    for filename in all_files:\n",
    "        try:\n",
    "            df = pd.read_csv(filename, encoding='latin-1')\n",
    "            standard_df = pd.DataFrame()\n",
    "            file_basename = os.path.basename(filename).lower()\n",
    "\n",
    "            if file_basename == 'data-augmented.csv':\n",
    "                if 'labels' in df.columns and 'text' in df.columns:\n",
    "                    standard_df['label'] = df['labels']\n",
    "                    standard_df['text'] = df['text']\n",
    "                else:\n",
    "                    print(f\"--> Skipping file: '{os.path.basename(filename)}'. Required columns 'labels' and 'text' not found.\")\n",
    "                    continue\n",
    "\n",
    "            elif file_basename == 'sms_spam.csv':\n",
    "                if 'label' in df.columns and 'sms' in df.columns:\n",
    "                    standard_df['label'] = df['label']\n",
    "                    standard_df['text'] = df['sms']\n",
    "                else:\n",
    "                    print(f\"--> Skipping file: '{os.path.basename(filename)}'. Required columns 'label' and 'sms' not found.\")\n",
    "                    continue\n",
    "\n",
    "            elif file_basename == 'sms_text.csv':\n",
    "                if 'label' in df.columns and 'data' in df.columns:\n",
    "                    standard_df['label'] = df['label']\n",
    "                    standard_df['text'] = df['data']\n",
    "                else:\n",
    "                    print(f\"--> Skipping file: '{os.path.basename(filename)}'. Required columns 'label' and 'data' not found.\")\n",
    "                    continue\n",
    "            \n",
    "            elif 'v1' in df.columns and 'v2' in df.columns:\n",
    "                standard_df['label'] = df['v1']\n",
    "                standard_df['text'] = df['v2']\n",
    "            \n",
    "            else:\n",
    "                print(f\"--> Skipping file: '{os.path.basename(filename)}'. Did not match any known file formats.\")\n",
    "                continue\n",
    "\n",
    "            label_map = {\n",
    "                'ham': 0, 'spam': 1, '0': 0, '1': 1,\n",
    "                'normal': 0, 'legitimate': 0\n",
    "            }\n",
    "            standard_df['label'] = standard_df['label'].astype(str).str.lower().map(label_map)\n",
    "            standard_df.dropna(inplace=True)\n",
    "            standard_df['label'] = standard_df['label'].astype(int)\n",
    "            \n",
    "            df_list.append(standard_df)\n",
    "            print(f\"--> Successfully loaded and processed '{os.path.basename(filename)}', adding {len(standard_df)} rows.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"--> Error processing file '{os.path.basename(filename)}': {e}\")\n",
    "\n",
    "    if not df_list:\n",
    "        print(\"\\nError: No data could be loaded from any of the files. Exiting.\")\n",
    "        exit()\n",
    "\n",
    "\n",
    "    master_df = pd.concat(df_list, ignore_index=True)\n",
    "    print(f\"\\nTotal combined rows: {len(master_df):,}\")\n",
    "\n",
    "\n",
    "    master_df.drop_duplicates(subset=['text'], inplace=True)\n",
    "    print(f\"Rows after removing duplicate text entries: {len(master_df):,}\")\n",
    "\n",
    "    master_df = master_df.sample(frac=1).reset_index(drop=True)\n",
    "    print(\"Final dataset shuffled.\")\n",
    "\n",
    "\n",
    "    # ---  CODE TO DISPLAY CLASS DISTRIBUTION (MOVED TO CORRECT LOCATION) ---\n",
    "    print(\"\\n--- Final Class Distribution ---\")\n",
    "\n",
    "\n",
    "    # Use value_counts() on the 'label' column before it's renamed\n",
    "    class_counts = master_df['label'].value_counts()\n",
    "    \n",
    "    ham_count = class_counts.get(0, 0)\n",
    "    spam_count = class_counts.get(1, 0)\n",
    "    \n",
    "    print(f\"Ham (0):  {ham_count:,}\")\n",
    "    print(f\"Spam (1): {spam_count:,}\")\n",
    "    print(\"--------------------------------\\n\")\n",
    "    # --------------------------------------------------------------------------\n",
    "\n",
    "    # Rename columns to v1 and v2 for the final output file\n",
    "    master_df.rename(columns={'label': 'v1', 'text': 'v2'}, inplace=True)\n",
    "\n",
    "    try:\n",
    "        # Save the final DataFrame to a new CSV file\n",
    "        master_df.to_csv(output_path, index=False)\n",
    "        print(f\"Successfully saved the cleaned dataset to '{output_path}' with columns v1 and v2.\")\n",
    "    except Exception as e:\n",
    "        print(f\"\\nError: Could not save the file. Reason: {e}\")\n",
    "    return master_df\n",
    "\n",
    "\n",
    "# Now the function handles everything internally: cleaning, counting, and saving.\n",
    "df = load_and_standardize_datasets(DATASET_DIRECTORY, OUTPUT_FILENAME)\n",
    "\n",
    "\n",
    "\n",
    "# The final print statement outside the function now correctly uses 'df'.\n",
    "print(\"\\n--- Final DataFrame Head (with v1, v2 columns) ---\")\n",
    "print(df.head(10))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.13.7)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
